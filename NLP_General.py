# -*- coding: utf-8 -*-
"""NLP_General

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JmO_WWbXfgI0hDRfapAkjt-c0-UW0TCA
"""

from nltk.tokenize import sent_tokenize, word_tokenize

import nltk
nltk.download('punkt')

text = """Besides professional life, I am also a wine enthusiast. I've been fortunate enough to visit some of the top wine producing regions in the world in the last number of years, including the Napa and Sonoma valleys in California"""

for i in word_tokenize(text):
  print(i)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

import nltk
nltk.download('stopwords')

text = "Besides professional life, I am also a wine enthusiast. I've been fortunate enough to visit some of the top wine producing regions in the world in the last number of years, including the Napa and Sonoma valleys in California,"
stop = set(stopwords.words("english"))

words = word_tokenize(text)
filtered_sentence = []

for w in words:
  if w not in stop:
    filtered_sentence.append(w)

print(stop)
print(filtered_sentence)

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps = PorterStemmer()

text = ["Besides professional life, I am also a wine enthusiast. I've been fortunate enough to visit some of the top wine producing regions in the world in the last number of years, including the Napa and Sonoma valleys in California,"]

for w in text:
  print(ps.stem(w))

import nltk
from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer
nltk.download('state_union')
nltk.download('averaged_perceptron_tagger')

text = state_union.raw('2005-GWBush.txt')
sample = state_union.raw('2006-GWBush.txt')

custom_tokenizer = PunktSentenceTokenizer(text)

tokenized = custom_tokenizer.tokenize(sample)

def process_content():
  try:
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)
      print(tagged)

  except Exception as e:
    print(str(e))

process_content()

import nltk
from nltk.corpus import state_union
from nltk.tokenize import PunktSentenceTokenizer
nltk.download('state_union')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

train_text = state_union.raw("2005-GWBush.txt")
sample_text = state_union.raw("2006-GWBush.txt")

custom = PunktSentenceTokenizer(train_text)
tokenized = custom.tokenize(sample_text)

def process():
  try:
    for i in tokenized:
      words = nltk.word_tokenize(i)
      tagged = nltk.pos_tag(words)

      chunkGram = r"""Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}"""

      chunkParser = nltk.RegexpParser(chunkGram)
      chunked = chunkParser.parse(tagged)

      print(chunked)
      #chunked.draw()

  except Exception as e:
    print(str(e))

process()

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()
print(lemmatizer.lemmatize("catss"))

from nltk.corpus import wordnet

syns = wordnet.synsets("cat")

print(syns[0].name())
print(syns[0].lemmas()[0].name())
print(syns[0].definition())
print(syns[0].examples())

synonyms = []
antonyms = []

for syn in wordnet.synsets("sleep"):
  for i in syn.lemmas():
    synonyms.append(i.name())
    if i.antonyms():
      antonyms.append(i.antonyms()[0].name())

print(set(synonyms))
print(set(antonyms))

w1 = wordnet.synset("north_korea.n.01")
w2 = wordnet.synset("south_korea.n.01")

print(w1)
print(w1.definition())
print(w1.wup_similarity(w2))

import nltk
import random
from nltk.corpus import movie_reviews
nltk.download('movie_reviews')

documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)
#print(documents[1])

all_words = []
for w in movie_reviews.words():
  all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)
print(all_words.most_common(15))
print(all_words["good"])

import nltk
import random
from nltk.corpus import movie_reviews
nltk.download('movie_reviews')

documents = [(list(movie_reviews.words(fileid)), category)
              for category in movie_reviews.categories()
              for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = []
for w in movie_reviews.words():
  all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)
word_features = list(all_words.keys())[:3000]

def find_features(document):
  words = set(document)
  features = {}
  for w in word_features:
    features[w] = {w in words}

  return features

print((find_features(movie_reviews.words('neg/cv000_29416.txt'))))
featuresets = [(find_features(a), b) for (a, b) in documents]

import nltk
import random
from nltk.corpus import movie_reviews

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = []

for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features

featuresets = [(find_features(rev), category) for (rev, category) in documents]

training_set = featuresets[:1000]
testing_set = featuresets[1000:]

classifier = nltk.NaiveBayesClassifier.train(training_set)

print("Classifier accuracy percent:",(nltk.classify.accuracy(classifier, testing_set))*100)
classifier.show_most_informative_features(15)

import nltk
import random
from nltk.corpus import movie_reviews
import pickle

documents = [(list(movie_reviews.words(fileid)), category)
             for category in movie_reviews.categories()
             for fileid in movie_reviews.fileids(category)]

random.shuffle(documents)

all_words = []

for w in movie_reviews.words():
    all_words.append(w.lower())

all_words = nltk.FreqDist(all_words)

word_features = list(all_words.keys())[:3000]

def find_features(document):
    words = set(document)
    features = {}
    for w in word_features:
        features[w] = (w in words)

    return features

featuresets = [(find_features(rev), category) for (rev, category) in documents]

training_set = featuresets[:1000]
testing_set = featuresets[1000:]

#classifier = nltk.NaiveBayesClassifier.train(training_set)

classifier_f = open("naivebayes.pickle", "rb")
classifier = pickle.load(classifier_f)
classifier_f.close()

print("Classifier accuracy percent:",(nltk.classify.accuracy(classifier, testing_set))*100)
classifier.show_most_informative_features(15)

#save_classifier = open("naivebayes.pickle", "wb")
#pickle.dump(classifier, save_classifier)
#save_classifier.close()